\documentclass[12pt]{article}

\usepackage[hyphens,spaces,obeyspaces]{url}
\usepackage{graphicx}
\graphicspath{ {./images/} }
    

\title{CMPUT 499 Literature Review}
\author{Monica Bui}

\begin{document}

\begin{titlepage}
    \centering
    \large
    \vspace{1cm}
    CMPUT 499: Mining Sofware Repositories \\
    \vspace{1cm}
    Literature Review \\
    \vspace{1cm}
    Monica Bui
\end{titlepage}

% What is the issue of paper? Main goals and motivation behind paper + questions? Methods of research? Results? Conclusion with personal opinion (good/bad/limitations).

% Literature review discusses published inforamtion in subject area
% Focus on summarizing source and introducing my own understandings of topic

\newpage
\section{Introduction}
% Give idea of the topic of the literature review, patterns + theme

\paragraph{}
Third-party software libraries and Application Programming Interfaces (APIs) offer a way for developers
to use existing features and integrate them within their projects without having to re-invent the wheel.
There are many such libraries to use for different programming languages and for different purposes e.g. encryption, text communications etc.
Although there is a lot of flexibility when it comes to choosing the appropriate library,
it is also conflicting to figure out which one is best for your needs. 
There are a variety of factors involved such as security and developer support that will affect your project's overall functionality.
In this review, we look at several diferent articles to help analyze various properties of libraries and 
to determine a reliable method of comparison between them.

\section{Article Review}

% Format by theme 
% Recommenders, Badges, API Comparisons by Metrics, Usage, etc.

\subsection{Library Recommenders}
\paragraph{}
One problem is researching for new analogical libraries to use that has similar functionalities 
to the ones you already know of. Chen's et. al \cite{analogical} paper discusses their library recommender 
that consumes as input a list of libraries from community resources such as blogs and Q\&A sites like StackOverflow \cite{stackoverflow} 
and outputs a list of recommended libraries in a different language of choice that offers similar features to the input language argument. 

This is implemented by first mining tags from questions posted online on StackOverflow \cite{stackoverflow}.
These tags are split into two knowledge bases: relational and categorical. Respectively, relational knowledge is how pairs of
tags are correlated to each other e.g. Java and JUnit, while categorical knowledge consists of 
how tags are grouped into categories such as language, operating system, concept, or library. 
Both these bases are analyzed by NLP to ensure efficient extraction of data and are sub categorized correctly.
Having these seperated tag categories and relatonships between tags often mentioned together provides 
a simpler way to recommend a new library. 
With the database in place, users can then search for recomendations through the built
web application called \textit{SimilarTech} \cite{similartech}.

While trying out the application myself, I found that search results would yield for libraries only mentioned in specific contexts.
Axios \cite{axios} a popular Javascript HTTP API should output expected recomendations 
like the Requests \cite{requests} module for Python but the actual output printed was empty.
Axios \cite{axios} itself is mentioned around many situations like REST, APIs, and HTTP requests making it difficult to target in what circumstance
it should be recommended in.

While the number of languages it can suggest libraries for is limited to 5, the precision metric is impressive
with 1 language at 81\% and with 5 being at 67\% demonstrating its potential to grow in the future.

\paragraph{}
Going back to the key problem of deciding on the right library to use, Uddin's et. al \cite{opinerarticle} article
highlights their approach to this by looking at personal developer opinions on different resources
and how it affects the reader's decision. The sentiment behind this can be used to indicate if its 
a positive, questionable, or a negative API to use. The tool created, Opiner \cite{opiner} is a summarization engine
that examines these opinions and evaluates its sentiment to see if the API should be recommended through 
displaying ratings on API traits in addition to organizing top opinions for both positive and negative sides. 

The backend of the application web crawls through a Q\&A site, StackOverflow \cite{stackoverflow} to extract answer
information surrounding an API topic. This data is used to help discover when an API in referenced and what kinds of opinions
are associated with an API by "finding its nearest API mention" \cite{opinerarticle} through an API name, link, or code said in the same sentence. 
This combination of data is then used to summarize common aspects developers care about such as usability and performance 
by finding their interests through a survey.

Using Opiner \cite{opiner} through a small research study evaluated on 2 seperate groups of particpant choices to see if 
just StackOverflow \cite{stackoverflow} alone would work for selection of an API or using both StackOverflow and Opiner would be better.
We view that developers are more confident in their selection with both tools versus just StackOverflow alone. 

For correctness, Opiner gives an overall 90\% precision metric on library recommendations that demonstrates that this data is indeed reliable.
A weakness is that although the evaluation of the study proves useful, having 100\% as a research study result for using both StackOverflow and Opiner
is difficult to generalize for a larger population. 

\subsection{Github Badges}
\paragraph{}
With many open source software to use, it's difficult to pin point which is worth your time to contribute to and/or integrate with your project.
Trockman et. al \cite{githubbadges} looks at the in depth quality of a package by analyzing repository badges 
eg. Figure \ref{phpbadge} that maintainers display on their README file.
Trockman not only suggests that badges are signals that makes internal software aspects more transparent 
but also "may impact users' and contributers' decision making" \cite{githubbadges}.
Game like elements known as \textit{Gamification} are not explicitly embedded into these badges 
but in reality motivate users to contribute higher quality code to increase the signal power displayed by these visuals. 
The key questions here are, \textit{What are the most common badges and what does displayong them intend to signal?} and 
\textit{To what degree do badges correlate with qualities that developers expect?} \cite{githubbadges}.
To examine the impact of badges, Node Package Modules (npm) \cite{npm} is used as the research repository 
as it's the largest online collection of Javascript packages. 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth,height=6cm,keepaspectratio=true]{gnrephpbadges}
    \caption{
        Example badges in a open source PHP repository \protect\cite{badgeimage}
    }
    \label{phpbadge}
\end{figure}

% Research method analysis
Data is collected by mining all npm \cite{npm} packages and then keeping those with metadata and a public Github \cite{github} repository. 
They extract the badges through the git history associated with 
the README file by matching the regular expression for a badge insertion then further classifying them into specific categories
such as quality assurance, popularity, dependencies etc. which each have different signaling intentions.
With their developer and maintainer survey insights, they develop sub questions to validate if the badges exert the correct qualities as intended. 
For each type of signal (dependencies, popularity, test suite, and quality pull requests), they look at 
impact before and after badge adoption with 8 months on both sides through longitudinal analysis, statistical regressions
to see how it is correlated with their key questions, and any underlying indicators that the badge may not overly express at first glance.

% Results and conclusion
The results suggest that displaying badges highly correlate with better code practices specifically higher test coverage, updated dependencies, and a increase in quality code.
However, overwhelming your repository with badges loses its intended signaling effect thus turns away users leading to a decrease in downloads,
with 5 being the best number of badges to show off.
Assessment dynamic signals that are more costly to produce are more reliable than static conventional badges 
that display trivial information easily found on the page.

% My opinions
With their survey design research method that collected software metrics from contributers and maintainers, having a low response rate of 15.3\% 
is challenging to generalize that these results will stay reliable for future studies.

% look for main motivation behind project, goals, why they looked only at these prospects.
% evidence for why this method is better

\subsection{Metrics}
\paragraph{}
% API popularity and migration
With software libaries evolving at a rapid speed, upgrading to the latest version can be cumbersome for developers
as challenges of backwards incompatability and deciding on new APIs for your project prove to be a problem.
Hora et. al \cite{apiwave} dicusses these 2 obstacles and implements a web application \textit{apiwave} \cite{apiwavewebsite} to mitigate this and
highlight API popularity and migration in depth with the former measured by the number of users using the service.

The research method takes the Git history of a repository as input and outputs information about the API's popularity metric 
and method of migration with code snippets as examples of this process.
Based on insertions and deletions in the resulting Git diff, we can detect which lines have been modified for migration 
and update the popularity statistic by 1 suggesting that the old library lost a follower and the new library gained one.
Mining import statement diffs assist with which APIs have been added or removed.
The client side of \textit{apiwave} \cite{apiwavewebsite} can present a library at many levels including specific interface lookup eg. java.util vs java.util.Map.
\textit{Apiwave} also displays addition and deletions statistics, an overall popularity graph, 
and code snippets that highlights recommended libraries to transfer over to based on diffs.

These results indicate different popularity trends and can answer real life StackOverflow \cite{stackoverflow} 
questions regarding API migration in testing with actual human answers. 
This helps to recommend the best software to use for developers based on compatability through migration
and the number of others who support this -- popularity.

Even though there is a high number of vistors and page views, it is currently limited to the Java language even though there are many lanugages out there
such as Javascript and Python that host readily available open source libaries. 


\paragraph{}
Selecting the best suited library for your work is often difficult as you need to make sure that they are not only reliable but also contain your desired features.
Mora et al. \cite{metrics} creates a method of comparison between libraries on a set criteria of metrics to help developers
compare and contrast between different software. 
From the given motivational example where a developer had trouble picking an API because of characteristics they were unaware of such as lack of community support,
Mora suggests that having a single website that presents these quantative metrics would lead to a simpler library decision.
Key questions to look at are how to compile this dataset, and finding metrics that express the underlying quality of the library that developers care about.

Data is collected from Stack Overflow \cite{stackoverflow}, a Q\&A resource, with source code and issues found on GitHub \cite{github} all to provide
diversified content to see the perspective of API qualities at all angles. 
For each metric a detailed description is given on it portrays, how this is extracted by using tools such as BOA \cite{boa} and issue trackers
or calculated using previous related research from other authors, and how it affects developers' decisions.
Specifically, Mora et al.\cite{metrics} examines in depth 3 metrics that created different challenges not mentioned in previous articles using a set of 60 Java libraries from different domains as a guideline.
BOA \cite{boa} is used to measure \textit{popularity} such that if a Java file contains an import statement, then we can conclude that the project is using that particular API.
Based on the number of import statements, we can see which libraries are the most popular for each domain e.g Junit for testing.
For \textit{migration}, Mora et al.\cite{metrics} uses another author's dataset to conclude that even if a library is very popular does not mean migration is also simpler as shown with Junit.
With \textit{Performance and Security}, the number of bugs related to that helps to measure this metric if it's a reliable API.
However, bug reports are diffcult to categorize as the bug description itself may not be overly stated in the report.
Supervised classifers is then used to label these bugs properly.

The web application displays detailed information all at one place to showcase the different quantitative metrics for each API. 
Although there is no audience feedback within the article, Mora et al. \cite{metrics} gives specific future prospects on how
to improve the application looking into survey feedback and dynamic updates to the information being displayed. 

The study is limited to further generalization to show that quanitative measures truly work without specifying any testing in the article. 

\section{Conclusion}
% Summarize key points
% Decide based on results from article, the method of reseach to dive into with my project
\paragraph{}
Deciding between different APIs in various libraries and frameworks proves to be challenging. 
Chen et al. \cite{analogical} and Udin et al. \cite{opinerarticle} look at explicitly recommending libraries through their web application with the latter targeting on opinion sentiment. 
Trockman et al. \cite{githubbadges} examines the impact of GitHub \cite{github} badges and whether they influence developer choices.
Finally, Hora et. al \cite{apiwave} and Mora et. al \cite{metrics} analyze quantitative statistics on API metrics with the former focusing on popularity and migration.

\paragraph{}
For future software metric research based on popular metrics found by Mora et. al \cite{metrics}
and Trockman et al. \cite{githubbadges}, we hope to create high quality, assessment Github \cite{github} badges
based on the most popular metrics to help recommend the best libraries for users through first
glance at the README file. Spending more effort on these assessment badges in addition to taking the most popular metrics 
will ensure that the impact on code quality increases
but does not overload the user with too much information on the page. 

\newpage
\bibliographystyle{IEEEtran}
\bibliography{literature_review}
\end{document}